{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "193V0MqzgHaz",
        "outputId": "1929d450-79bc-48fc-8fd7-723781576b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We create the model\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(460, 700, 3))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(8, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "C65RYxlYgfiI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pull train and test images into two np arrays\n",
        "#import os\n",
        "#from PIL import Image\n",
        "#base = \"/content/drive/MyDrive/cancer_400/\"\n",
        "#train_dir = base + \"train/\"\n",
        "#test_dir = base + \"test/\"\n",
        "# create train and test full path names\n",
        "#dir_names = [\"adenosis/\", \"ductal_ca/\", \"fibroadenoma/\", \"lobular_ca/\", \"mucinous_ca/\",\n",
        "#             \"papillary_ca/\", \"phylloides/\", \"tubular_ad/\"]\n",
        "#train_image_paths = []\n",
        "#test_image_paths = []\n",
        "#for d in dir_names:\n",
        "#  print('proc dir:', d)\n",
        "#  path = train_dir + d\n",
        "#  print('len', len(os.listdir(path)))\n",
        "\n",
        "#  path = test_dir + d\n",
        "#  print('len', len(os.listdir(path)))"
      ],
      "metadata": {
        "id": "7uHNsUwspEMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pull train and test images into two np arrays\n",
        "import os\n",
        "from PIL import Image\n",
        "base = \"/content/drive/MyDrive/cancer_400/\"\n",
        "train_dir = base + \"train/\"\n",
        "test_dir = base + \"test/\"\n",
        "# create train and test full path names\n",
        "dir_names = [\"adenosis/\", \"ductal_ca/\", \"fibroadenoma/\", \"lobular_ca/\", \"mucinous_ca/\",\n",
        "             \"papillary_ca/\", \"phylloides/\", \"tubular_ad/\"]\n",
        "train_image_paths = []\n",
        "test_image_paths = []\n",
        "for d in dir_names:\n",
        "  print('proc dir:', d)\n",
        "  path = train_dir + d\n",
        "  for fn in os.listdir(path):\n",
        "    train_image_paths.append(path + fn)\n",
        "  path = test_dir + d\n",
        "  for fn in os.listdir(path):\n",
        "    test_image_paths.append(path + fn)\n",
        "\n",
        "# read file paths, flatten, add to list of flattened images\n",
        "train_flattened = []\n",
        "print('23 len train flat', len(train_flattened))\n",
        "test_flattened = []\n",
        "print('25 len test flat', len(test_flattened))\n",
        "\n",
        "print('opening train images')\n",
        "image_count = 0\n",
        "for count, path in enumerate(train_image_paths):\n",
        "  a = np.asarray(Image.open(path))\n",
        "  if count == 0:\n",
        "    print('a shape:', a.shape)\n",
        "  try:\n",
        "    assert a.shape == (460, 700, 3)\n",
        "  except Exception:\n",
        "    print(count, a.shape, path)\n",
        "    continue  # skip this incorrect shaped image\n",
        "  af = a.flatten()\n",
        "  train_flattened.append(af)\n",
        "  image_count += 1\n",
        "  if count % 50 == 0:\n",
        "    print(count, end=',')\n",
        "print('train final count', image_count)\n",
        "print()\n",
        "print('opening test images')\n",
        "for count, path in enumerate(test_image_paths):\n",
        "  a = np.asarray(Image.open(path))\n",
        "  assert a.shape == (460, 700, 3)\n",
        "  af = a.flatten()\n",
        "  test_flattened.append(af)\n",
        "  if count % 50 == 0:\n",
        "    print(count, end=',')\n",
        "print('test final count', count)\n",
        "print()\n",
        "\n",
        "print('generate two np arrays')\n",
        "# stack flattened arrays into single array\n",
        "train_images = np.hstack(train_flattened)\n",
        "test_images = np.hstack(test_flattened)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-7WWIYOj6R0",
        "outputId": "5ff93ed2-35f1-441f-801c-87e57707ad2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "proc dir: adenosis/\n",
            "proc dir: ductal_ca/\n",
            "proc dir: fibroadenoma/\n",
            "proc dir: lobular_ca/\n",
            "proc dir: mucinous_ca/\n",
            "proc dir: papillary_ca/\n",
            "proc dir: phylloides/\n",
            "proc dir: tubular_ad/\n",
            "23 len train flat 0\n",
            "25 len test flat 0\n",
            "opening train images\n",
            "a shape: (460, 700, 3)\n",
            "0,50,100,150,200,250,300,350,375 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-001.png\n",
            "376 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-003.png\n",
            "377 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-002.png\n",
            "378 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-007.png\n",
            "379 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-008.png\n",
            "380 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-004.png\n",
            "381 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-009.png\n",
            "382 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-005.png\n",
            "383 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-010.png\n",
            "384 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-006.png\n",
            "385 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-013.png\n",
            "386 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-012.png\n",
            "387 (456, 700, 3) /content/drive/MyDrive/cancer_400/train/papillary_ca/SOB_M_PC-14-12465-400-011.png\n",
            "400,450,500,550,train final count 587\n",
            "\n",
            "opening test images\n",
            "0,50,100,150,test final count 199\n",
            "\n",
            "generate two np arrays\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_flattened)\n",
        "for n, im in enumerate(train_flattened):\n",
        "  try:\n",
        "    assert im.shape[0] == 460*700*3\n",
        "  except Exception:\n",
        "    print('n when stopped:', n)\n",
        "    break"
      ],
      "metadata": {
        "id": "gW9dKqB3rINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = np.hstack(train_flattened)\n",
        "tf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brfaG59BwRMm",
        "outputId": "e4833afc-da17-4dac-ccbe-f719d24c81bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(567042000,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "567042000 / (460*700*3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfWg8MBPwqYn",
        "outputId": "9d20b246-ec10-4303-d7b5-2813159fc1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "587.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIBTh_mIpUb5",
        "outputId": "574751ef-3877-4639-f8e4-4168bde7e086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(567042000,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create train and test label np arrays\n",
        "train_labels = np.array([0]*75+[1]*75+[2]*75+[3]*75+[4]*75+[5]*62+[6]*75+[7]*75)\n",
        "test_list= []\n",
        "for im_class in range(8):\n",
        "  test_list += [im_class]*25\n",
        "test_labels = np.array(test_list)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDWtahgiqvyt",
        "outputId": "a84d9f07-8640-4424-fee9-c92f2c8e5836"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(587,)\n",
            "(200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WE fit the model\n",
        "train_images = train_images.reshape((600-13, 460, 700, 3))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((200, 460, 700, 3))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels, epochs=2, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1We3XHxhCAJ",
        "outputId": "ac19fa67-d410-4084-9f57-72f84e245f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "19/19 [==============================] - 395s 20s/step - loss: 24.0667 - accuracy: 0.1806\n",
            "Epoch 2/2\n",
            "13/19 [===================>..........] - ETA: 2:03 - loss: 2.0808 - accuracy: 0.2596"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "47040000/(460*700*3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnaKPQIWiCpv",
        "outputId": "cfe3c6e8-93cb-4476-a4f6-fae0a6c4ecb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48.69565217391305"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "400*400*3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJIlQZnBscWo",
        "outputId": "e1b77597-b715-43fd-c357-9d0cdce819ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480000"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "480000/78400"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGfj12GYslVE",
        "outputId": "a8c3b857-aa9b-4829-8011-59e5a400fbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.122448979591836"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "gQEpNcV1iC3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# second model\n",
        "inputs = keras.Input(shape=(460, 700, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(8, activation=\"softmax\")(x)\n",
        "model2 = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "1hMskZCPsai0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((600-13, 460, 700, 3))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((200, 460, 700, 3))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "model2.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "model2.fit(train_images, train_labels, epochs=30, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnNj9URV35iM",
        "outputId": "d2c37796-3c57-4ba0-a9a8-20091c7d224a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model2.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "uy5-JTmf4nXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# third model\n",
        "inputs = keras.Input(shape=(460, 700, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(8, activation=\"softmax\")(x)\n",
        "model3 = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "AjYcU5YK1SnV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "model3.fit(train_images, train_labels, epochs=4, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "abllaiEx37aB",
        "outputId": "cee69bad-a833-48fa-e4bb-dc1033039afb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5e72c896aec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     metrics=[\"accuracy\"])\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1653\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1654\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 567042000\n  y sizes: 587\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model3.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "9HCAgqsL4o5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base2 = \"/content/drive/MyDrive/Ductal/\"\n",
        "#base3 = \"/content/drive/MyDrive/Phylloides/\"\n",
        "#predict2_dir = base + \"Ductal/\"\n",
        "#test_dir = base + \"test/\"\n",
        "# create train and test full path names\n",
        "#dir_names1 = [\"Ductal/\"]\n",
        "\n",
        "predict_image_paths = []\n",
        "#test_image_paths = []\n",
        "#for d in dir_names1:\n",
        "#  print('proc dir:', d)\n",
        "path = base2\n",
        "for fn in os.listdir(path):\n",
        "  predict_image_paths.append(path + fn)\n",
        "#  path = test_dir + d\n",
        "#  for fn in os.listdir(path):\n",
        "#   test_image_paths.append(path + fn)\n",
        "\n",
        "# read file paths, flatten, add to list of flattened images\n",
        "predict_flattened = []\n",
        "\n",
        "print('opening train images')\n",
        "\n",
        "image_count = 0\n",
        "for count, path in enumerate(predict_image_paths):\n",
        "  a = np.asarray(Image.open(path))\n",
        "  if count == 0:\n",
        "    print('a shape:', a.shape)\n",
        "  try:\n",
        "    assert a.shape == (460, 700, 3)\n",
        "  except Exception:\n",
        "    print(count, a.shape, path)\n",
        "    continue  # skip this incorrect shaped image\n",
        "  af = a.flatten()\n",
        "  train_flattened.append(af)\n",
        "  image_count += 1\n",
        "#  if count % 50 == 0:\n",
        "#    print(count, end=',')\n",
        "print('train final count', image_count)\n",
        "\n",
        "print('generate two np arrays')\n",
        "# stack flattened arrays into single array\n",
        "predict_images = np.hstack(train_flattened)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc6zuBV999Jo",
        "outputId": "49f5c888-0ee8-413f-8afc-9324ccfe5851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opening train images\n",
            "a shape: (460, 700, 3)\n",
            "train final count 8\n",
            "generate two np arrays\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use this to make predictions, unknown_images is size N x 460 x 700 x 3\n",
        "model.predict(predict_images)"
      ],
      "metadata": {
        "id": "PW7xOR5L5wOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}